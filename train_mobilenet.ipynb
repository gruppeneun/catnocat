{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of train_mobilenet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwL2SNC0Kvun"
      },
      "source": [
        "Sources:\n",
        "* https://github.com/PracticalDL/Practical-Deep-Learning-Book/tree/master/code/chapter-3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q89KplLZlPZm"
      },
      "source": [
        "# Preparations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-_u-ZDBNNH4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDKObaBwNH2T"
      },
      "source": [
        "!pip install tensorflow-gpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ2-WDlvRS90"
      },
      "source": [
        "import os\n",
        "\n",
        "# Location of Zip File\n",
        "drive_path = '/content/drive/My Drive/Colab Notebooks/catnocat/data.zip'\n",
        "local_path = '/content/catnocat'\n",
        "\n",
        "if not os.path.isdir(local_path):\n",
        "  os.mkdir(local_path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZpGkLldRp_h"
      },
      "source": [
        "# Copy the zip file from the google drive\n",
        "!cp '{drive_path}' '{local_path}'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfMcEN2YRrzo"
      },
      "source": [
        "# Navigate to the copied file and unzip it quietly\n",
        "os.chdir(local_path)\n",
        "!unzip -q 'data.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4A7NmEV3NrIc"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBrL2bUhNF_X"
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Flatten, Dense, GlobalAveragePooling2D, Dropout\n",
        "from keras.applications import MobileNet\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import itertools\n",
        "import matplotlib.image as mpimg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcyrhQF2nU4X"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "import datetime, os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ei0vw-Q4Nku3"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEaUtJq50MBs"
      },
      "source": [
        "# Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7AxacXvNF_b"
      },
      "source": [
        "IMG_WIDTH = 224\n",
        "IMG_HEIGHT = 224\n",
        "NUM_CLASSES = 2\n",
        "\n",
        "image_size = (IMG_WIDTH, IMG_HEIGHT)\n",
        "batch_size = 32\n",
        "\n",
        "num_epochs = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSq6aHwmlXWg"
      },
      "source": [
        "# Read the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUiNEwWRNF_Z"
      },
      "source": [
        "data_path = '/content/catnocat'\n",
        "\n",
        "train_data_path = os.path.join(data_path, 'train')\n",
        "valid_data_path = os.path.join(data_path, 'valid')\n",
        "# holytest_data_path = os.path.join(data_path, 'holytest')\n",
        "\n",
        "print(\"Path to training set: \", train_data_path)\n",
        "print(\"Path to validation set: \", valid_data_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFIeQw1bNF_c"
      },
      "source": [
        "# Set Data Generator for training, testing and validation.\n",
        "\n",
        "# Note for testing, set shuffle = false (For proper Confusion matrix)\n",
        "train_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
        "train_dataset = train_datagen.flow_from_directory(train_data_path,\n",
        "                                                  target_size=image_size,\n",
        "                                                  batch_size=batch_size,\n",
        "                                                  class_mode='categorical',\n",
        "                                                  shuffle=True)\n",
        "\n",
        "valid_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
        "valid_dataset = valid_datagen.flow_from_directory(valid_data_path,\n",
        "                                                  target_size=image_size,\n",
        "                                                  batch_size=batch_size,\n",
        "                                                  class_mode='categorical',\n",
        "                                                  shuffle=False)\n",
        "\n",
        "# test_datagen = ImageDataGenerator(rescale=1/255)\n",
        "# test_dataset = test_datagen.flow_from_directory(holytest_data_path,\n",
        "#                                                 target_size=image_size,\n",
        "#                                                 batch_size=batch_size,\n",
        "#                                                 class_mode='categorical',\n",
        "#                                                 shuffle=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnDDmXhdmwYs"
      },
      "source": [
        "# Display the MobileNet CNN Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M39LXAnfz6Ll"
      },
      "source": [
        "base_model = MobileNet(weights='imagenet',\n",
        "                       include_top=False,\n",
        "                       input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))  # imports the mobilenet model and discards the last 1000 neuron layer.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve8wVzFa0AOo"
      },
      "source": [
        "# check the architecture\n",
        "for i, layer in enumerate(base_model.layers):\n",
        "    print(i, layer.name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2UYLKg3D7Ge"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqMWopxJD-p1"
      },
      "source": [
        "def display_accuracy_plot(model_results):\n",
        "  acc = model_results.history['accuracy']\n",
        "  val_acc = model_results.history['val_accuracy']\n",
        "\n",
        "  loss = model_results.history['loss']\n",
        "  val_loss = model_results.history['val_loss']\n",
        "\n",
        "  epochs_range = range(len(model_results.history['accuracy']))\n",
        "\n",
        "  plt.figure(figsize=(15, 6))\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "  plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "  plt.legend(loc='lower right')\n",
        "  plt.title('Training and Validation Accuracy')\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.plot(epochs_range, loss, label='Training Loss')\n",
        "  plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "  plt.legend(loc='upper right')\n",
        "  plt.title('Training and Validation Loss')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGL5OTfImqM3"
      },
      "source": [
        "# Compile & Train the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FCr7NlGIuiX"
      },
      "source": [
        "## Model Variant 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wTvlOclIwlP"
      },
      "source": [
        "model_variant = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wD1qIvQ2IyGM"
      },
      "source": [
        "def model0_maker():\n",
        "    base_model = MobileNet(include_top=False,\n",
        "                           input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
        "    \n",
        "    for layer in base_model.layers[:]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    input = Input(shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
        "    custom_model = base_model(input)\n",
        "    custom_model = GlobalAveragePooling2D()(custom_model)\n",
        "    #custom_model = Dense(64, activation='relu')(custom_model)\n",
        "    #custom_model = Dropout(0.5)(custom_model)\n",
        "    predictions = Dense(NUM_CLASSES, activation='softmax')(custom_model)\n",
        "    return Model(inputs=input, outputs=predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iho3hmECI0Ki"
      },
      "source": [
        "# Compile and train the model.\n",
        "model = model0_maker()\n",
        "\n",
        "model.compile(optimizer='Adam', \n",
        "              loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Adam optimizer\n",
        "# loss function will be categorical cross entropy\n",
        "# evaluation metric will be accuracy\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyjE1mh-I3Lm"
      },
      "source": [
        "logdir = os.path.join(\"logs\", f'variant{model_variant}_'+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3O5mHJSI5Ge"
      },
      "source": [
        "epochs = num_epochs\n",
        "step_size_train = train_dataset.n // train_dataset.batch_size\n",
        "history = model.fit(train_dataset,\n",
        "                    steps_per_epoch=step_size_train,\n",
        "                    epochs=epochs,\n",
        "                    validation_data=valid_dataset,\n",
        "                    callbacks=[tensorboard_callback]\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj2gvUbpI7cA"
      },
      "source": [
        "# Save the model\n",
        "model.save(f'cats_mobilenet_variant{model_variant}.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsuCsq36I9g0"
      },
      "source": [
        "display_accuracy_plot(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fEPIEnn2Aq3"
      },
      "source": [
        "## Model Variant 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nn7paXu3sO5"
      },
      "source": [
        "model_variant = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4bRhyUR0pQ7"
      },
      "source": [
        "def model1_maker():\n",
        "    base_model = MobileNet(include_top=False,\n",
        "                           input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
        "    \n",
        "    for layer in base_model.layers[:]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    input = Input(shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
        "    custom_model = base_model(input)\n",
        "    custom_model = GlobalAveragePooling2D()(custom_model)\n",
        "    custom_model = Dense(64, activation='relu')(custom_model)\n",
        "    custom_model = Dropout(0.5)(custom_model)\n",
        "    predictions = Dense(NUM_CLASSES, activation='softmax')(custom_model)\n",
        "    return Model(inputs=input, outputs=predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdU_5h8gNF_g"
      },
      "source": [
        "# Compile and train the model.\n",
        "model = model1_maker()\n",
        "\n",
        "model.compile(optimizer='Adam', \n",
        "              loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Adam optimizer\n",
        "# loss function will be categorical cross entropy\n",
        "# evaluation metric will be accuracy\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv7BGTLGrHR5"
      },
      "source": [
        "Initialize Tensorboard:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HG2TrkhHq_2E"
      },
      "source": [
        "logdir = os.path.join(\"logs\", f'variant{model_variant}_'+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8G1OR2TtrKWG"
      },
      "source": [
        "Start the training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgukmL5LNF_g"
      },
      "source": [
        "epochs = num_epochs\n",
        "step_size_train = train_dataset.n // train_dataset.batch_size\n",
        "history = model.fit(train_dataset,\n",
        "                    steps_per_epoch=step_size_train,\n",
        "                    epochs=epochs,\n",
        "                    validation_data=valid_dataset,\n",
        "                    callbacks=[tensorboard_callback]\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejD80fsS3xqM"
      },
      "source": [
        "# Save the model\n",
        "model.save(f'cats_mobilenet_variant{model_variant}.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRwTak5NEttc"
      },
      "source": [
        "display_accuracy_plot(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FmhaFjM2KB7"
      },
      "source": [
        "## Model Variant 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMcYwFLE3gvp"
      },
      "source": [
        "model_variant = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4dMhH1UBhnn"
      },
      "source": [
        "def model2_maker():\n",
        "    base_model = MobileNet(include_top=False,\n",
        "                           input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
        "    \n",
        "    # we retrain the last layers with indicies > 79\n",
        "    n = 79\n",
        "    for layer in base_model.layers[:n]:\n",
        "        layer.trainable = False\n",
        "    for layer in base_model.layers[n:]:\n",
        "        layer.trainable = True\n",
        "\n",
        "\n",
        "    custom_model = base_model.output\n",
        "    custom_model = GlobalAveragePooling2D()(custom_model)\n",
        "    # we add dense layers so that the model can learn more complex functions and classify for better results.\n",
        "    custom_model = Dense(1024, \n",
        "                         activation='relu')(custom_model)  \n",
        "    custom_model = Dense(1024, activation='relu')(custom_model) # dense layer 2\n",
        "    custom_model = Dense(512, activation='relu')(custom_model)  # dense layer 3\n",
        "    preds = Dense(NUM_CLASSES, activation='softmax')(custom_model)  # final layer with softmax activation\n",
        "    model = Model(inputs=base_model.input, outputs=preds)\n",
        "    return model    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9II27sa2JBE"
      },
      "source": [
        "model = model2_maker()\n",
        "\n",
        "# Compile and train the model.\n",
        "model.compile(optimizer='Adam', \n",
        "              loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Adam optimizer\n",
        "# loss function will be categorical cross entropy\n",
        "# evaluation metric will be accuracy\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZFPegF62yx0"
      },
      "source": [
        "logdir = os.path.join(\"logs\", f'variant{model_variant}_'+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a6Jdgde23DX"
      },
      "source": [
        "epochs = num_epochs\n",
        "step_size_train = train_dataset.n // train_dataset.batch_size\n",
        "history = model.fit(train_dataset,\n",
        "                    steps_per_epoch=step_size_train,\n",
        "                    epochs=epochs,\n",
        "                    validation_data=valid_dataset,\n",
        "                    callbacks=[tensorboard_callback]\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgFyHF-T3Wgn"
      },
      "source": [
        "# Save the model\n",
        "model.save(f'cats_mobilenet_variant{model_variant}.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG4iGFe1EsTB"
      },
      "source": [
        "display_accuracy_plot(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bEZLdU-DCaq"
      },
      "source": [
        "## Model Variant 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlgZ1cy1DEyM"
      },
      "source": [
        "model_variant = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nq_j_SANDHtK"
      },
      "source": [
        "def model3_maker():\n",
        "    base_model = MobileNet(include_top=False,\n",
        "                           input_shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
        "    \n",
        "    # we retrain the last layers with indicies > 79\n",
        "    n = 79\n",
        "    for layer in base_model.layers[:n]:\n",
        "        layer.trainable = False\n",
        "    for layer in base_model.layers[n:]:\n",
        "        layer.trainable = True\n",
        "\n",
        "\n",
        "    custom_model = base_model.output\n",
        "    custom_model = GlobalAveragePooling2D()(custom_model)\n",
        "    # we add dense layers so that the model can learn more complex functions and classify for better results.\n",
        "    custom_model = Dense(1024, \n",
        "                         activation='relu')(custom_model)  \n",
        "    custom_model = Dense(1024, activation='relu')(custom_model) # dense layer 2\n",
        "    custom_model = Dense(512, activation='relu')(custom_model)  # dense layer 3\n",
        "    custom_model = Dropout(0.5)(custom_model)\n",
        "    preds = Dense(NUM_CLASSES, activation='softmax')(custom_model)  # final layer with softmax activation\n",
        "    model = Model(inputs=base_model.input, outputs=preds)\n",
        "    return model "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uipYhFCIDOHX"
      },
      "source": [
        "model = model3_maker()\n",
        "\n",
        "# Compile and train the model.\n",
        "model.compile(optimizer='Adam', \n",
        "              loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Adam optimizer\n",
        "# loss function will be categorical cross entropy\n",
        "# evaluation metric will be accuracy\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2tXSdAuDRnV"
      },
      "source": [
        "logdir = os.path.join(\"logs\", f'variant{model_variant}_'+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4xLyvB4DTZA"
      },
      "source": [
        "epochs = num_epochs\n",
        "step_size_train = train_dataset.n // train_dataset.batch_size\n",
        "history = model.fit(train_dataset,\n",
        "                    steps_per_epoch=step_size_train,\n",
        "                    epochs=epochs,\n",
        "                    validation_data=valid_dataset,\n",
        "                    callbacks=[tensorboard_callback]\n",
        "                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzGx0OEUDVlw"
      },
      "source": [
        "# Save the model\n",
        "model.save(f'cats_mobilenet_variant{model_variant}.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UNVuyXXElQ0"
      },
      "source": [
        "# Visualize the training and validation accuracy over the epochs:\n",
        "display_accuracy_plot(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-uZiwpQ2ppH"
      },
      "source": [
        "# Summarize Results with TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BjBPY9YtcUD"
      },
      "source": [
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFPFM6kxmek8"
      },
      "source": [
        "# Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ricEpbPRFXx8"
      },
      "source": [
        "# load the \"best\" model to visualize some predictions based on the validation set\n",
        "model = tf.keras.models.load_model('cats_mobilenet_variant3.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCNcNoMqmg9z"
      },
      "source": [
        "## Prediction - Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAQOkL5nNF_j"
      },
      "source": [
        "# PREDICTION\n",
        "Y_pred = model.predict_generator(valid_dataset, step_size_train + 1)\n",
        "y_pred = np.argmax(Y_pred, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rue6V1thNF_k"
      },
      "source": [
        "# Confusion Matrix and Classification Report\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(valid_dataset.classes, y_pred))\n",
        "print('Classification Report')\n",
        "class_names = list(valid_dataset.class_indices.keys())\n",
        "print(classification_report(valid_dataset.classes, y_pred, target_names=class_names))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ikOcDKUkNF_k"
      },
      "source": [
        "# Plot the confusion matrix. Set Normalize = True/False\n",
        "def plot_confusion_matrix(cm, classes, normalize=True, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        cm = np.around(cm, decimals=2)\n",
        "        cm[np.isnan(cm)] = 0.0\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lbIz9N1NF_k"
      },
      "source": [
        "# Confusion Matrix\n",
        "print('Confusion Matrix')\n",
        "cm = confusion_matrix(valid_dataset.classes, y_pred)\n",
        "plot_confusion_matrix(cm, class_names, title='Confusion Matrix')\n",
        "\n",
        "# Print Classification Report\n",
        "\n",
        "print('Classification Report')\n",
        "print(classification_report(valid_dataset.classes, y_pred, target_names=class_names))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVIx7OQZmWn-"
      },
      "source": [
        "## Predictions - Explore classified images from Validation Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdz7VS-KNF_l"
      },
      "source": [
        "# PREDICTIONS 2\n",
        "ground_truth = valid_dataset.classes\n",
        "print(ground_truth[:10])\n",
        "print(len(ground_truth))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SEFueaXNF_l"
      },
      "source": [
        "# Then we get the predictions. This will be a list of probability values that express how confident\n",
        "# the model is about the presence of each category in each image. This step might take several minutes.\n",
        "\n",
        "predictions = model.predict_generator(valid_dataset,\n",
        "                                      steps=None)\n",
        "print(predictions[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFIgnnhgNF_l"
      },
      "source": [
        "prediction_table = {}\n",
        "for index, val in enumerate(predictions):\n",
        "    index_of_highest_probability = np.argmax(val)\n",
        "    value_of_highest_probability = val[index_of_highest_probability]\n",
        "    prediction_table[index] = [\n",
        "        value_of_highest_probability,\n",
        "        index_of_highest_probability,\n",
        "        ground_truth[index]\n",
        "    ]\n",
        "assert len(predictions) == len(ground_truth) == len(prediction_table)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFPPhhP-NF_l"
      },
      "source": [
        "def reverse_dict(class_dict):\n",
        "  reversed = {}\n",
        "  for key, value in class_dict.items():\n",
        "    reversed[value] = key\n",
        "  return reversed\n",
        "\n",
        "def get_images_with_sorted_probabilities(prediction_table,\n",
        "                                         get_highest_probability,\n",
        "                                         label,\n",
        "                                         number_of_items,\n",
        "                                         only_false_predictions=False):\n",
        "    sorted_prediction_table = [(k, prediction_table[k])\n",
        "                               for k in sorted(prediction_table,\n",
        "                                               key=prediction_table.get,\n",
        "                                               reverse=get_highest_probability)\n",
        "                               ]\n",
        "    result = []\n",
        "    for index, key in enumerate(sorted_prediction_table):\n",
        "        image_index, [probability, predicted_index, gt] = key\n",
        "        if predicted_index == label:\n",
        "            if only_false_predictions == True:\n",
        "                if predicted_index != gt:\n",
        "                    result.append(\n",
        "                        [image_index, [probability, predicted_index, gt]])\n",
        "            else:\n",
        "                result.append(\n",
        "                    [image_index, [probability, predicted_index, gt]])\n",
        "    return result[:number_of_items]\n",
        "\n",
        "\n",
        "def plot_images(filenames, distances, classification_txt, title_txt):\n",
        "    images = []\n",
        "    for filename in filenames:\n",
        "        images.append(mpimg.imread(filename))\n",
        "    plt.figure(figsize=(20, 24))\n",
        "    columns = 5\n",
        "    for i, image in enumerate(images):\n",
        "        ax = plt.subplot(int(len(images) / columns + 1), columns, i + 1)\n",
        "        ax.set_title(\"\\n\\n\" + filenames[i].split(\"/\")[-1] + \n",
        "                     \"\\n\" + classification_txt + \n",
        "                     \"\\nprobability=\" + str(float(\"{0:.2f}\".format(distances[i])))\n",
        "                     )\n",
        "        plt.suptitle(title_txt, fontsize=20, fontweight='bold')\n",
        "        plt.axis('off')\n",
        "        plt.imshow(image)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "filenames = valid_dataset.filenames\n",
        "class_dict = reverse_dict(valid_dataset.class_indices)\n",
        "\n",
        "def display(sorted_indices, title_txt):\n",
        "    similar_image_paths = []\n",
        "    distances = []\n",
        "    for name, value in sorted_indices:\n",
        "        [probability, predicted_index, gt] = value\n",
        "        if predicted_index == gt:\n",
        "            classification_txt = \"CORRECT\"\n",
        "        else:\n",
        "            classification_txt = \"WRONG\"\n",
        "        classification_txt = \"{}\\nground truth: {}\\npredicted: {}\".format(classification_txt, \n",
        "                                                                 class_dict[gt].upper(), \n",
        "                                                                 class_dict[predicted_index].upper())\n",
        "        similar_image_paths.append(os.path.join(valid_data_path, filenames[name]))\n",
        "        distances.append(probability)\n",
        "    plot_images(similar_image_paths, distances, classification_txt, title_txt)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4pNP0NRNF_l"
      },
      "source": [
        "img_list = get_images_with_sorted_probabilities(prediction_table,\n",
        "                                                             get_highest_probability=True,\n",
        "                                                             label=0,\n",
        "                                                             number_of_items=20,\n",
        "                                                             only_false_predictions=False)\n",
        "message = 'Images with highest probability of containing cats'\n",
        "display(img_list, message)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PxZSBQhNF_n"
      },
      "source": [
        "img_list = get_images_with_sorted_probabilities(prediction_table,\n",
        "                                                             get_highest_probability=True,\n",
        "                                                             label=1,\n",
        "                                                             number_of_items=20,\n",
        "                                                             only_false_predictions=False)\n",
        "message = 'Images with highest probability of containing no cats'\n",
        "display(img_list, message)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDJWUQxsNF_n"
      },
      "source": [
        "img_list = get_images_with_sorted_probabilities(prediction_table,\n",
        "                                                        get_highest_probability=False,\n",
        "                                                        label=1,\n",
        "                                                        number_of_items=20,\n",
        "                                                        only_false_predictions=True)\n",
        "message = 'Wrongly classified images, with lowest probability'\n",
        "display(img_list, message)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enTyf7mNNF_n"
      },
      "source": [
        "img_list = get_images_with_sorted_probabilities(prediction_table,\n",
        "                                                        get_highest_probability=False,\n",
        "                                                        label=0,\n",
        "                                                        number_of_items=20,\n",
        "                                                        only_false_predictions=True)\n",
        "message = 'Wrongly classified images, with lowest probability'\n",
        "display(img_list, message)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkhJ8ubBLKcV"
      },
      "source": [
        "# Evaluate the Model using the Holy Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILEfOxPQNF_n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}